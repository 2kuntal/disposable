#!/usr/bin/env python3

import json
import re
import urllib.request

sources = {
    'line': ["https://gist.githubusercontent.com/adamloving/4401361/raw/66688cf8ad890433b917f3230f44489aa90b03b7",
             "https://gist.githubusercontent.com/michenriksen/8710649/raw/d42c080d62279b793f211f0caaffb22f1c980912",
             "https://raw.githubusercontent.com/wesbos/burner-email-providers/7f3c191e876790b7a01b053a8319a59183037993/emails.txt"],
    'json': ["https://raw.githubusercontent.com/ivolo/disposable-email-domains/master/index.json"]
}

domain_regex = re.compile(r'^[a-z\d-]{,63}(\.[a-z\d-]{,63})+$')
domains = {}
out_file = 'domains'
supported_formats = list(sources.keys())

skip = ['qq.com']
banlist = ['0815.ru',
           '0815.su',
           'a45.in',
           'abcmail.email',
           'bund.us',
           'bundes-li.ga',
           'cachedot.net',
           'discard.email',
           'discardmail.com',
           'discardmail.de',
           'fiifke.de',
           'freundin.ru',
           'hartbot.de',
           'hulapla.de',
           'inoutmail.de',
           'inoutmail.eu',
           'inoutmail.info',
           'inoutmail.net',
           'loadby.us',
           'manifestgenerator.com',
           'nonspam.eu',
           'nonspammer.de',
           'noref.in',
           'pfui.ru',
           's0ny.net',
           'showslow.de',
           'spambog.com',
           'spambog.de',
           'spambog.ru',
           'spamstack.net',
           'sweetxxx.de',
           'teewars.org',
           'webuser.in',
           'yapped.net']


def process_url(url, fmt, encoding='utf-8', timeout=3):
    if fmt not in supported_formats:
        return

    data = ''
    try:
        data = urllib.request.urlopen(url, timeout=timeout).read() or ''
    except Exception as err:
        print('WRN Fetching URL {0} failed, see error: {1}'.format(url, err))
        return

    lines = []
    if fmt == 'list':
        lines = data.split()
    elif fmt == 'json':
        raw = json.loads(data.decode(encoding))
        if not isinstance(raw, list):
            print(
                'WRN This URL does not contain a JSON array: {0}'.format(url))
            return
        lines = list(filter(lambda line: line and isinstance(line, str), raw))

    lines = [line.lower().strip(' .,;') for line in lines]
    lines = list(filter(lambda line: domain_regex.match(line), lines))

    global domains
    for line in lines:
        domains[line] = None

if __name__ == '__main__':
    # build domains dict
    for fmt in supported_formats:
        for src in sources[fmt]:
            process_url(src, fmt)
    # add custom black and white list
    for domain in banlist:
        domains[domain] = None
    for domain in skip:
        domains.pop(domain, None)

    # read and compare to current (old) domains file
    old_domains = {}
    with open(out_file + '.txt') as ff:
        for line in ff.read().split():
            old_domains[line] = None
    added = list(
        filter(lambda domain: domain not in old_domains, domains.keys()))
    removed = list(
        filter(lambda domain: domain not in domains, old_domains.keys()))

    print('Fetched {0} domains'.format(len(domains)))
    print(' - {0} domain(s) added'.format(len(added)))
    print(' - {0} domain(s) removed'.format(len(removed)))

    # stop if nothing has changed
    if len(added) == len(removed) == 0:
        exit()

    # write new list to file(s)
    domains = list(domains.keys())
    domains.sort()
    with open(out_file + '.txt', 'w') as ff:
        ff.write('\n'.join(domains))
    with open(out_file + '.json', 'w') as ff:
        ff.write(json.dumps(domains))
